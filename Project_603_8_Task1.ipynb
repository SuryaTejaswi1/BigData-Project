{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1:\n",
    "<b>Objective : The objective is to perform a country-wise analysis of emissions spanning all sectors. This analysis aims to pinpoint   the primary sectors contributing to emissions globally, along with their associated gases.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> `Initiating Spark Session`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"green\"> Spark application is running in standalone mode on the local machine</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.2.4-bin-hadoop2.7\\\\spark-3.2.4-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark job user: Surya\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Check_User\").getOrCreate()\n",
    "\n",
    "# Get the user running the Spark job\n",
    "user = spark.sparkContext.sparkUser()\n",
    "print(\"Spark job user:\", user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Application ID: local-1701873538400\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Get_Application_ID\").getOrCreate()\n",
    "\n",
    "# Get the Spark application ID\n",
    "app_id = spark.sparkContext.applicationId\n",
    "print(\"Spark Application ID:\", app_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Master: local[*]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()\n",
    "\n",
    "spark_master = spark._sc._conf.get(\"spark.master\")\n",
    "print(\"Spark Master:\", spark_master)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Files are stored in local HDFS </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       note: please use \"yarn jar\" to launch\r\n",
      "                             YARN applications, not this command.\r\n",
      "Found 9 items\r\n",
      "-rw-r--r--   1 Surya supergroup       1158 2023-11-29 06:07 /user/username/project_data/Data/HDFS Commands.txt\r\n",
      "drwxr-xr-x   - Surya supergroup          0 2023-11-29 06:12 /user/username/project_data/Data/agriculture\r\n",
      "drwxr-xr-x   - Surya supergroup          0 2023-11-29 06:12 /user/username/project_data/Data/buildings\r\n",
      "drwxr-xr-x   - Surya supergroup          0 2023-11-29 06:12 /user/username/project_data/Data/fluorinated_gases\r\n",
      "drwxr-xr-x   - Surya supergroup          0 2023-11-29 06:12 /user/username/project_data/Data/fossil_fuel_operations\r\n",
      "drwxr-xr-x   - Surya supergroup          0 2023-12-02 01:32 /user/username/project_data/Data/manufacturing\r\n",
      "drwxr-xr-x   - Surya supergroup          0 2023-11-29 06:12 /user/username/project_data/Data/mineral_extraction\r\n",
      "drwxr-xr-x   - Surya supergroup          0 2023-12-02 01:33 /user/username/project_data/Data/power\r\n",
      "drwxr-xr-x   - Surya supergroup          0 2023-11-29 06:12 /user/username/project_data/Data/waste\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Hadoop project data directory\n",
    "hadoop_directory = \"/user/username/project_data/Data\"\n",
    "\n",
    "# Run the 'hadoop fs -ls' command using subprocess\n",
    "try:\n",
    "    ls_command = f\"hadoop fs -ls {hadoop_directory}\"\n",
    "    output = subprocess.check_output(ls_command, shell=True).decode('utf-8')\n",
    "    print(output)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error executing command: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "\n",
    "import os\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Task 1\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Filtering for files starting with name Country_ and storing their respective file paths </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the CSV files\n",
    "schema = StructType([\n",
    "    StructField(\"iso3_country\", StringType(), True),\n",
    "    StructField(\"start_time\", TimestampType(), True),\n",
    "    StructField(\"end_time\", TimestampType(), True),\n",
    "    StructField(\"original_inventory_sector\", StringType(), True),\n",
    "    StructField(\"gas\", StringType(), True),\n",
    "    StructField(\"emissions_quantity\", DoubleType(), True),\n",
    "    StructField(\"emissions_quantity_units\", StringType(), True),\n",
    "    StructField(\"temporal_granularity\", StringType(), True),\n",
    "    StructField(\"created_date\", StringType(), True),\n",
    "    StructField(\"modified_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Define the HDFS path\n",
    "hdfs_path = \"hdfs://localhost:9000/user/username/project_data/Data/\"\n",
    "\n",
    "\n",
    "# List of folders to explore \n",
    "folders = [\"agriculture\", \"buildings\", \"fluorinated_gases\", \"fossil_fuel_operations\", \n",
    "           \"manufacturing\", \"mineral_extraction\", \"power\", \"waste\"]\n",
    "\n",
    "file_paths = []  # Initialize list to store file paths\n",
    "\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(hdfs_path, folder)\n",
    "    files = spark.read.format(\"csv\").schema(schema).load(folder_path)\n",
    "    \n",
    "    # Extract filename from the file path using input_file_name()\n",
    "    files_with_path = files.withColumn(\"filename\", input_file_name())\n",
    "    \n",
    "    # Filter files with filenames starting with \"country_\"\n",
    "    country_files = files_with_path.filter(files_with_path[\"filename\"].contains(\"country_\"))\n",
    "    \n",
    "    # Collect the paths of the identified files\n",
    "    country_paths = country_files.select(input_file_name()).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Append the paths to the file_paths list\n",
    "    file_paths.extend(country_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hdfs://localhost:9000/user/username/project_data/Data/agriculture/country_manure-management_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/agriculture/country_synthetic-fertilizer-application_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/agriculture/country_enteric-fermentation_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/agriculture/country_cropland-fires_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/agriculture/country_rice-cultivation_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/agriculture/country_other-agricultural-soil-emissions_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/buildings/country_residential-and-commercial-onsite-fuel-usage_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/buildings/country_other-onsite-fuel-usage_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/fluorinated_gases/country_fluorinated-gases_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/fossil_fuel_operations/country_oil-and-gas-production-and-transport_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/fossil_fuel_operations/country_other-fossil-fuel-operations_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/fossil_fuel_operations/country_solid-fuel-transformation_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/fossil_fuel_operations/country_oil-and-gas-refining_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/fossil_fuel_operations/country_coal-mining_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/manufacturing/country_steel_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/manufacturing/country_chemicals_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/manufacturing/country_aluminum_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/manufacturing/country_pulp-and-paper_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/manufacturing/country_other-manufacturing_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/manufacturing/country_cement_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/mineral_extraction/country_rock-quarrying_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/mineral_extraction/country_sand-quarrying_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/mineral_extraction/country_bauxite-mining_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/mineral_extraction/country_copper-mining_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/mineral_extraction/country_iron-mining_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/power/country_other-energy-use_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/power/country_electricity-generation_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/waste/country_incineration-and-open-burning-of-waste_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/waste/country_wastewater-treatment-and-discharge_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/waste/country_biological-treatment-of-solid-waste-&-biogenic_emissions.csv',\n",
       " 'hdfs://localhost:9000/user/username/project_data/Data/waste/country_solid-waste-disposal_emissions.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------------------+-------------------------+---------+------------------+------------------------+--------------------+--------------------+--------------------+\n",
      "|iso3_country|         start_time|           end_time|original_inventory_sector|      gas|emissions_quantity|emissions_quantity_units|temporal_granularity|        created_date|       modified_date|\n",
      "+------------+-------------------+-------------------+-------------------------+---------+------------------+------------------------+--------------------+--------------------+--------------------+\n",
      "|         MNE|2020-01-01 00:00:00|2020-12-31 00:00:00|     residential-and-c...|      co2|              null|                  tonnes|                null|2022-09-06 12:39:...|                null|\n",
      "|         AND|2020-01-01 00:00:00|2020-12-31 00:00:00|     residential-and-c...|      co2|              null|                  tonnes|                null|2022-09-06 12:39:...|                null|\n",
      "|         SHN|2019-01-01 00:00:00|2019-12-31 00:00:00|     residential-and-c...|      n2o|           7.41E-5|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|\n",
      "|         SSD|2019-01-01 00:00:00|2019-12-31 00:00:00|     residential-and-c...|      co2|              null|                  tonnes|                null|2022-09-06 12:39:...|                null|\n",
      "|         SSD|2019-01-01 00:00:00|2019-12-31 00:00:00|     residential-and-c...|      n2o|              null|                  tonnes|                null|2022-09-06 12:39:...|                null|\n",
      "|         VAT|2019-01-01 00:00:00|2019-12-31 00:00:00|     residential-and-c...|      co2|              null|                  tonnes|                null|2022-09-06 12:39:...|                null|\n",
      "|         ABW|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|      co2|       154132.0457|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|\n",
      "|         ABW|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|      n2o|         21.958181|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|\n",
      "|         AFG|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|      co2|       210721.8796|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|\n",
      "|         AFG|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|      n2o|31.200474999999997|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|\n",
      "|         AGO|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|      co2|       3257421.128|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|\n",
      "|         AGO|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|      n2o|         26.101249|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|\n",
      "|         AIA|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|      co2|         14.128882|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|\n",
      "|         AIA|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|      n2o|           0.00255|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|\n",
      "|         MNE|2020-01-01 00:00:00|2020-12-31 00:00:00|     residential-and-c...|      n2o|              null|                  tonnes|                null|2022-09-06 12:39:...|                null|\n",
      "|         ALB|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|      co2|       763798.3528|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|\n",
      "|         ALB|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|      n2o|         63.034717|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|\n",
      "|         MNE|2020-01-01 00:00:00|2020-12-31 00:00:00|     residential-and-c...|      ch4|              null|                  tonnes|                null|2022-09-06 12:39:...|                null|\n",
      "|         MNE|2020-01-01 00:00:00|2020-12-31 00:00:00|     residential-and-c...|co2e_20yr|              null|                  tonnes|                null|2022-09-06 12:39:...|                null|\n",
      "|         NER|2020-01-01 00:00:00|2020-12-31 00:00:00|     residential-and-c...|      ch4|32162.196522000002|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|\n",
      "+------------+-------------------+-------------------+-------------------------+---------+------------------+------------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "#selected_columns = [\"iso3_country\", \"original_inventory_sector\", \"gas\", \"emissions_quantity\"]\n",
    "\n",
    "# Read and merge the identified CSV files into a single DataFrame with selected columns\n",
    "dataframes = []  \n",
    "\n",
    "for file_path in file_paths:\n",
    "    \n",
    "    df_chunks = spark.read.format(\"csv\").schema(schema).option(\"header\", \"true\").load(file_paths)\n",
    "        \n",
    "    # Select desired columns and handle missing values\n",
    "    #selected_df = df_chunks.select(selected_columns)\n",
    "\n",
    "    #selected_df.show()\n",
    "        \n",
    "    # Append the processed DataFrame to the list\n",
    "    dataframes.append(df_chunks)   \n",
    "        \n",
    "# Merge all DataFrames into a single DataFrame\n",
    "merged_df = reduce(DataFrame.unionAll, dataframes)\n",
    "\n",
    "# Show the merged DataFrame\n",
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8509810"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7578663"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = merged_df.dropna(subset=[\"emissions_quantity\"])\n",
    "cleaned_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Mapping the subsectors with their respective sectors </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "#distinct_sectors = cleaned_df.select(\"original_inventory_sector\").distinct()\n",
    "#distinct_sectors.show()\n",
    "\n",
    "sector_mapping = {\n",
    "    \"incineration-and-open-burning-of-waste\": \"Waste\",\n",
    "    \"other-fossil-fuel-operations\": \"Fossil Fuels\",\n",
    "    \"residential-and-commercial-onsite-fuel-usage\": \"Buildings\",\n",
    "    \"wastewater-treatment-and-discharge\": \"Waste\",\n",
    "    \"cropland-fires\": \"Agriculture\",\n",
    "    \"biological-treatment-of-solid-waste-&-biogenic\": \"Waste\",\n",
    "    \"oil-and-gas-production-and-transport\": \"Fossil Fuels\",\n",
    "    \"other-manufacturing\": \"Manufacturing\",\n",
    "    \"other-onsite-fuel-usage\": \"Buildings\",\n",
    "    \"other-energy-use\": \"Power\",\n",
    "    \"electricity-generation\": \"Power\",\n",
    "    \"solid-fuel-transformation\": \"Fossil Fuels\",\n",
    "    \"solid-waste-disposal\": \"Waste\",\n",
    "    \"synthetic-fertilizer-application\": \"Agriculture\",\n",
    "    \"other-agricultural-soil-emissions\": \"Agriculture\",\n",
    "    \"oil-and-gas-refining\": \"Fossil Fuels\",\n",
    "    \"enteric-fermentation\": \"Agriculture\",\n",
    "    \"rice-cultivation\": \"Agriculture\",\n",
    "    \"coal-mining\": \"Fossil Fuels\",\n",
    "    \"cement\": \"Manufacturing\",\n",
    "    \"steel\": \"Manufacturing\",\n",
    "    \"manure-management\": \"Agriculture\",\n",
    "    \"pulp-and-paper\": \"Manufacturing\",\n",
    "    \"fluorinated-gases\": \"Fluorinated Gas\",\n",
    "    \"rock-quarrying\": \"Mineral Extraction\",\n",
    "    \"bauxite-mining\": \"Mineral Extraction\",\n",
    "    \"sand-quarrying\": \"Mineral Extraction\",\n",
    "    \"copper-mining\": \"Mineral Extraction\",\n",
    "    \"aluminum\": \"Manufacturing\",\n",
    "    \"chemicals\": \"Manufacturing\",\n",
    "    \"iron-mining\": \"Mineral Extraction\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------------------+-------------------------+---+------------------+------------------------+--------------------+--------------------+--------------------+---------+\n",
      "|iso3_country|         start_time|           end_time|original_inventory_sector|gas|emissions_quantity|emissions_quantity_units|temporal_granularity|        created_date|       modified_date|   sector|\n",
      "+------------+-------------------+-------------------+-------------------------+---+------------------+------------------------+--------------------+--------------------+--------------------+---------+\n",
      "|         SHN|2019-01-01 00:00:00|2019-12-31 00:00:00|     residential-and-c...|n2o|           7.41E-5|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|\n",
      "|         ABW|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|       154132.0457|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|\n",
      "|         ABW|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|         21.958181|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|\n",
      "|         AFG|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|       210721.8796|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|\n",
      "|         AFG|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|31.200474999999997|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|\n",
      "+------------+-------------------+-------------------+-------------------------+---+------------------+------------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Construct the SQL expression for mapping original_inventory_sector to corresponding sector\n",
    "sql_expr = \"CASE \"\n",
    "for sector, mapping in sector_mapping.items():\n",
    "    sql_expr += f\"WHEN original_inventory_sector = '{sector}' THEN '{mapping}' \"\n",
    "\n",
    "# Add the 'Other' condition at the end\n",
    "sql_expr += \"ELSE 'Other' END\"\n",
    "\n",
    "# Apply the SQL expression to create the 'sector' column\n",
    "cleaned_df = cleaned_df.withColumn(\"sector\", expr(sql_expr))\n",
    "\n",
    "cleaned_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- iso3_country: string (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- end_time: timestamp (nullable = true)\n",
      " |-- original_inventory_sector: string (nullable = true)\n",
      " |-- gas: string (nullable = true)\n",
      " |-- emissions_quantity: double (nullable = true)\n",
      " |-- emissions_quantity_units: string (nullable = true)\n",
      " |-- temporal_granularity: string (nullable = true)\n",
      " |-- created_date: string (nullable = true)\n",
      " |-- modified_date: string (nullable = true)\n",
      " |-- sector: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Processing supporting file  to get the country name from the iso3_codes</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------------+------------+------------------+-------------------+\n",
      "|       Country|Alpha-2 code|Alpha-3 code|Numeric code|Latitude (average)|Longitude (average)|\n",
      "+--------------+------------+------------+------------+------------------+-------------------+\n",
      "|   Afghanistan|        \"AF\"|       \"AFG\"|         \"4\"|              \"33\"|               \"65\"|\n",
      "|       Albania|        \"AL\"|       \"ALB\"|         \"8\"|              \"41\"|               \"20\"|\n",
      "|       Algeria|        \"DZ\"|       \"DZA\"|        \"12\"|              \"28\"|                \"3\"|\n",
      "|American Samoa|        \"AS\"|       \"ASM\"|        \"16\"|        \"-14.3333\"|             \"-170\"|\n",
      "|       Andorra|        \"AD\"|       \"AND\"|        \"20\"|            \"42.5\"|              \"1.6\"|\n",
      "+--------------+------------+------------+------------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the HDFS path to your CSV file\n",
    "hdfs_path = \"hdfs://localhost:9000/user/username/project_data/countries/countries_codes_and_coordinates.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "country_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(hdfs_path)\n",
    "\n",
    "country_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Alpha-2 code: string (nullable = true)\n",
      " |-- Alpha-3 code: string (nullable = true)\n",
      " |-- Numeric code: string (nullable = true)\n",
      " |-- Latitude (average): string (nullable = true)\n",
      " |-- Longitude (average): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_df.filter(col(\"Alpha-3 code\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.filter(col(\"iso3_country\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "#cleaning the data\n",
    "\n",
    "country_df = country_df.withColumn(\"Alpha-3 code\", regexp_replace(col(\"Alpha-3 code\"), '\"', ''))\n",
    "\n",
    "#country_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = cleaned_df.select(\"iso3_country\").exceptAll(country_df.select(\"Alpha-3 code\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Joining both the dataframes </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, trim\n",
    "\n",
    "\n",
    "# Rename columns in country_df for simplicity\n",
    "country_df = country_df.withColumnRenamed(\"Latitude (average)\", \"Latitude\") \\\n",
    "    .withColumnRenamed(\"Longitude (average)\", \"Longitude\")\n",
    "\n",
    "\n",
    "# Convert columns to lowercase for consistency in the join condition\n",
    "cleaned_df = cleaned_df.withColumn(\"iso3_country_lower\", lower(cleaned_df[\"iso3_country\"]))\n",
    "country_df = country_df.withColumn(\"Alpha-3 code_lower\", lower(country_df[\"Alpha-3 code\"]))\n",
    " \n",
    "joined_df1 = cleaned_df.join(country_df, trim(cleaned_df[\"iso3_country_lower\"]) == trim(country_df[\"Alpha-3 code_lower\"]), how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['iso3_country_lower', 'Alpha-2 code', 'Alpha-3 code', 'Numeric code', 'Alpha-3 code_lower']\n",
    "\n",
    "joined_df1 = joined_df1.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------------------+-------------------------+---+------------------+------------------------+--------------------+--------------------+--------------------+---------+--------------------+-----------+-----------+\n",
      "|iso3_country|         start_time|           end_time|original_inventory_sector|gas|emissions_quantity|emissions_quantity_units|temporal_granularity|        created_date|       modified_date|   sector|             Country|   Latitude|  Longitude|\n",
      "+------------+-------------------+-------------------+-------------------------+---+------------------+------------------------+--------------------+--------------------+--------------------+---------+--------------------+-----------+-----------+\n",
      "|         SHN|2019-01-01 00:00:00|2019-12-31 00:00:00|     residential-and-c...|n2o|           7.41E-5|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|Saint Helena, Asc...| \"-15.9333\"|     \"-5.7\"|\n",
      "|         ABW|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|       154132.0457|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|               Aruba|     \"12.5\"| \"-69.9667\"|\n",
      "|         ABW|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|         21.958181|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|               Aruba|     \"12.5\"| \"-69.9667\"|\n",
      "|         AFG|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|       210721.8796|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|         Afghanistan|       \"33\"|       \"65\"|\n",
      "|         AFG|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|31.200474999999997|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|         Afghanistan|       \"33\"|       \"65\"|\n",
      "|         AGO|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|       3257421.128|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|              Angola|    \"-12.5\"|     \"18.5\"|\n",
      "|         AGO|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|         26.101249|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|              Angola|    \"-12.5\"|     \"18.5\"|\n",
      "|         AIA|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|         14.128882|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|            Anguilla|    \"18.25\"| \"-63.1667\"|\n",
      "|         AIA|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|           0.00255|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|            Anguilla|    \"18.25\"| \"-63.1667\"|\n",
      "|         ALB|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|       763798.3528|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|             Albania|       \"41\"|       \"20\"|\n",
      "+------------+-------------------+-------------------+-------------------------+---+------------------+------------------------+--------------------+--------------------+--------------------+---------+--------------------+-----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df1 = joined_df1.withColumn(\"Latitude\", regexp_replace(col(\"Latitude\"), '\"', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Task1 = joined_df1.withColumn(\"Longitude\", regexp_replace(col(\"Longitude\"), '\"', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------------------+-------------------------+---+--------------------+------------------------+--------------------+--------------------+--------------------+---------+--------------------+---------+---------+\n",
      "|iso3_country|         start_time|           end_time|original_inventory_sector|gas|  emissions_quantity|emissions_quantity_units|temporal_granularity|        created_date|       modified_date|   sector|             Country| Latitude|Longitude|\n",
      "+------------+-------------------+-------------------+-------------------------+---+--------------------+------------------------+--------------------+--------------------+--------------------+---------+--------------------+---------+---------+\n",
      "|         SHN|2019-01-01 00:00:00|2019-12-31 00:00:00|     residential-and-c...|n2o|             7.41E-5|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|Saint Helena, Asc...| -15.9333|     -5.7|\n",
      "|         ABW|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|         154132.0457|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|               Aruba|     12.5| -69.9667|\n",
      "|         ABW|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|           21.958181|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|               Aruba|     12.5| -69.9667|\n",
      "|         AFG|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|         210721.8796|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|         Afghanistan|       33|       65|\n",
      "|         AFG|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|  31.200474999999997|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|         Afghanistan|       33|       65|\n",
      "|         AGO|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|         3257421.128|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|              Angola|    -12.5|     18.5|\n",
      "|         AGO|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|           26.101249|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|              Angola|    -12.5|     18.5|\n",
      "|         AIA|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|           14.128882|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|            Anguilla|    18.25| -63.1667|\n",
      "|         AIA|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|             0.00255|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|            Anguilla|    18.25| -63.1667|\n",
      "|         ALB|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|         763798.3528|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|             Albania|       41|       20|\n",
      "|         ALB|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|           63.034717|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|             Albania|       41|       20|\n",
      "|         NER|2020-01-01 00:00:00|2020-12-31 00:00:00|     residential-and-c...|ch4|  32162.196522000002|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|               Niger|       16|        8|\n",
      "|         ARE|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|   803545.6880000001|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|United Arab Emirates|       24|       54|\n",
      "|         ARE|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|            1.273448|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|United Arab Emirates|       24|       54|\n",
      "|         ARG|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|3.8603200900000006E7|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|           Argentina|      -34|      -64|\n",
      "|         ARG|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|         3988.265914|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|           Argentina|      -34|      -64|\n",
      "|         ARM|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|         1315044.422|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|             Armenia|       40|       45|\n",
      "|         ARM|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|           22.589083|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings|             Armenia|       40|       45|\n",
      "|         ATG|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|co2|          97609.6886|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings| Antigua and Barbuda|    17.05|    -61.8|\n",
      "|         ATG|2018-01-01 00:00:00|2018-12-31 00:00:00|     residential-and-c...|n2o|           13.891466|                  tonnes|                null|2022-09-06 12:39:...|2022-10-05 21:26:...|Buildings| Antigua and Barbuda|    17.05|    -61.8|\n",
      "+------------+-------------------+-------------------+-------------------------+---+--------------------+------------------------+--------------------+--------------------+--------------------+---------+--------------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Task1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['temporal_granularity', 'created_date', 'modified_date']\n",
    "\n",
    "Task1 = Task1.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Storing the parts of the output file (247 parts) in HDFS </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing in Parquet format\n",
    "Task1.write.format(\"parquet\").mode(\"ignore\").save(\"hdfs://localhost:9000/user/username/project_data/output_parquet\")\n",
    "\n",
    "# Writing in CSV format\n",
    "Task1.write.format(\"csv\").mode(\"ignore\").save(\"hdfs://localhost:9000/user/username/project_data/output_csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing as a single csv file in the local drive\n",
    "num_files = 1  # Number of output files\n",
    "\n",
    "Task1.coalesce(num_files) \\\n",
    "    .write \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"F:/project/Data/task1_output1_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
